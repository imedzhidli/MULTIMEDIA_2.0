{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvcgfmzRIwuT"
      },
      "source": [
        "# Лабораторная работа №6 (\"Проведение исследований с моделями классификации\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO3s54a5IwuV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn\n",
        "import torchvision.transforms\n",
        "import torchvision.models\n",
        "import torchinfo\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIJrVef2IwuW"
      },
      "source": [
        "## Выбор начальных условий"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVSBur10IwuW"
      },
      "source": [
        "### Выбор датасета"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3b1h5A-IwuW"
      },
      "source": [
        "Для обучения модели классификации изображений я использовал датасет [`Nfiniteai/product-masks-sample`](https://huggingface.co/datasets/Nfiniteai/product-masks-sample) с платформы Hugging Face.\n",
        "Он содержит фотореалистичные изображения, созданные на основе 3D-моделей предметов домашнего интерьера, таких как мебель и элементы декора. Высокое качество визуализации позволяет эффективно обучать модели для работы с реалистичными сценами.\n",
        "\n",
        "Цель работы — обучить модель выполнять задачи классификации, включая распознавание и сегментацию объектов на изображениях, их отделение от фона и категоризацию по типам (например, столы, стулья, светильники). Такой подход может быть применен для автоматизации систем учета товаров в розничной торговле, например, для упрощения инвентаризации в магазинах мебели или бытовой техники."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-TGKb2_IwuX"
      },
      "source": [
        "Скачаем датасет при помощи библиотеки \"datasets\"\n",
        "и покажем его краткое содержимое."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "449b7404bccc415cbdfc277afa149a23",
            "e6e93c1d0b524187a2fddb2587487eee",
            "cc215deb4955495a80e70007d2ea1799",
            "150f23aaf09a460b8481cf003bb9c013",
            "cc324b2f5fe245db80017f218453dbe2",
            "75e4d3b110ee413fa7481a77408f8d99",
            "c8fa548821a94833971edb7640742e2b",
            "43276c2eb4b141c8b077905567edff32",
            "2b78d96354bb4efd98902c4d7fb33c25",
            "7252c3df35c84c579c58ee647ccdc14e",
            "461e63824ac0423e882e2b914e557e38",
            "47e17b6ec9c44ec0958535a7c9e6f698",
            "f009e62abbba4922b4ccd18de446fee6",
            "98a85b05618e48b7aefda6e1c7402925",
            "03ac26970895426dbaa83b3e4d347c95",
            "5c41480fe18841dbb34392c4f161400b",
            "c751e718014b4df19c8bef879bdd8609",
            "81201bca46334c03ac814feccefa4c58"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-21T14:56:27.583536Z",
          "iopub.status.busy": "2025-04-21T14:56:27.583095Z",
          "iopub.status.idle": "2025-04-21T14:58:22.490120Z",
          "shell.execute_reply": "2025-04-21T14:58:22.488889Z",
          "shell.execute_reply.started": "2025-04-21T14:56:27.583513Z"
        },
        "id": "KDUfDbMcIwuX",
        "outputId": "d79d2486-8e26-4512-dde5-b357e822ac1e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "449b7404bccc415cbdfc277afa149a23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/3.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6e93c1d0b524187a2fddb2587487eee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00014.parquet:   0%|          | 0.00/345M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc215deb4955495a80e70007d2ea1799",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00014.parquet:   0%|          | 0.00/251M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "150f23aaf09a460b8481cf003bb9c013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00014.parquet:   0%|          | 0.00/283M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc324b2f5fe245db80017f218453dbe2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00014.parquet:   0%|          | 0.00/305M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e4d3b110ee413fa7481a77408f8d99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00004-of-00014.parquet:   0%|          | 0.00/298M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8fa548821a94833971edb7640742e2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00005-of-00014.parquet:   0%|          | 0.00/223M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43276c2eb4b141c8b077905567edff32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00006-of-00014.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b78d96354bb4efd98902c4d7fb33c25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00007-of-00014.parquet:   0%|          | 0.00/253M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7252c3df35c84c579c58ee647ccdc14e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00008-of-00014.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "461e63824ac0423e882e2b914e557e38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00009-of-00014.parquet:   0%|          | 0.00/275M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47e17b6ec9c44ec0958535a7c9e6f698",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00010-of-00014.parquet:   0%|          | 0.00/213M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f009e62abbba4922b4ccd18de446fee6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00011-of-00014.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98a85b05618e48b7aefda6e1c7402925",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00012-of-00014.parquet:   0%|          | 0.00/233M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03ac26970895426dbaa83b3e4d347c95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00013-of-00014.parquet:   0%|          | 0.00/198M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c41480fe18841dbb34392c4f161400b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "val-00000-of-00001.parquet:   0%|          | 0.00/222M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c751e718014b4df19c8bef879bdd8609",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2559 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81201bca46334c03ac814feccefa4c58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating val split:   0%|          | 0/151 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 2559\n",
            "Validation dataset size: 151\n",
            "Features:\n",
            "image_id: Value(dtype='string', id=None)\n",
            "image: Image(mode=None, decode=True, id=None)\n",
            "mask: Image(mode=None, decode=True, id=None)\n",
            "category: Value(dtype='string', id=None)\n",
            "bbox: Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)\n",
            "product_id: Value(dtype='string', id=None)\n",
            "scene_id: Value(dtype='string', id=None)\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.load_dataset('Nfiniteai/product-masks-sample')\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Validation dataset size: {len(dataset['val'])}\")\n",
        "\n",
        "print('Features:')\n",
        "for feature_name, feature_type in dataset['train'].features.items():\n",
        "    print(f'{feature_name}: {feature_type}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdSEw5MEIwuY"
      },
      "source": [
        "### Выбор метрик"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoFZ49CNIwuY"
      },
      "source": [
        "Для задачи классификации важно использовать метрики,\n",
        "которые будут отображать производительность модели\n",
        "с учетом особенностей данных, как, к примеру, несбалансированность данных.\n",
        "\n",
        "Итого, были выбраны следующие метрики для оценки обученной модели\n",
        "для решения задачи классификации:\n",
        "\n",
        "- **Accuracy** - эта метрика самая распространенная\n",
        "  и дает общее представление о доле правильно классифицированных объектов.\n",
        "  Она хорошо подходит для поверхностной оценки модели, однако не является\n",
        "  достаточно информативной при наличии особенностей в датасете,\n",
        "  таких как несбалансированность.\n",
        "\n",
        "- **Precision** - данная метрика показывает, на сколько модель\n",
        "  уверена в своих предсказаниях. Такая метрика важна в случаях,\n",
        "  когда мы заинтересованы в минимизации ложных срабатываний.\n",
        "\n",
        "- **Recall** - данная метрика показывает полноту обнаружения объектов каждого класса.\n",
        "  Для каждого класса она рассчитывается как отношение числа правильно классифицированных\n",
        "  объектов этого класса к общему числу объектов данного класса в выборке.\n",
        "\n",
        "- **F1-Score** - данная метрика представляет собой гармоническое среднее между\n",
        "  \"Precision\" и \"Recall\". Она является особенно полезной при наличии особенностей\n",
        "  в данных, таких как несбалансированность. Сама метрика учитывает\n",
        "  как ложные срабатывания, так и пропущенные случаи.\n",
        "\n",
        "Итого, использование перечисленного набора метрик поможет нам получить более точную оценку обученной модели и удостовериться, что при несбалансированности данных модель не будет просто выбирать самый частый класс."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAUQKS67IwuZ"
      },
      "source": [
        "## Создание бейзлайна и оценка качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHqpBgq8IwuZ"
      },
      "source": [
        "Задача бейзлайна состоит в создании простой начальной версии модели, которая будет выполнять роль отправной точки для дальнейших улучшений бейзлайна.\n",
        "\n",
        "Для этого сделаем минимальное преобразование скачанного датасета, чтобы он был пригоден для обучения модели. Для этого выполним нормализацию названий классов и уберем из тренировочной выборки те записи, в которых есть не предусмотренные в валидационной выборке классы. И уберем все лишние поля, которые не понадобятся при обучении, чтобы экономить ресурсы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T15:07:13.311519Z",
          "iopub.status.busy": "2025-04-21T15:07:13.309996Z",
          "iopub.status.idle": "2025-04-21T15:07:13.317638Z",
          "shell.execute_reply": "2025-04-21T15:07:13.316633Z",
          "shell.execute_reply.started": "2025-04-21T15:07:13.311482Z"
        },
        "id": "J-DT5ENEIwuZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "val_dataset = dataset['val']\n",
        "train_dataset = dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ed254a1d7f0240fe89b7cce711b0cf04",
            "dd174107e86548daa05a39a6b24034fd"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-21T15:07:15.405979Z",
          "iopub.status.busy": "2025-04-21T15:07:15.405029Z",
          "iopub.status.idle": "2025-04-21T15:08:00.751652Z",
          "shell.execute_reply": "2025-04-21T15:08:00.750778Z",
          "shell.execute_reply.started": "2025-04-21T15:07:15.405912Z"
        },
        "id": "RG-DXDTLIwuZ",
        "outputId": "9fa494bc-c0e3-492f-9a8c-ea6dbf1b20ff",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed254a1d7f0240fe89b7cce711b0cf04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd174107e86548daa05a39a6b24034fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2559 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def preprocess_category(category):\n",
        "    return category.lower().replace(' ', '_')\n",
        "\n",
        "def preprocess_category_batch(batch):\n",
        "    batch['category'] = [preprocess_category(el) for el in batch['category']]\n",
        "    return batch\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_category_batch,\n",
        "    load_from_cache_file=False,\n",
        "    batched=True,\n",
        "    batch_size=300,\n",
        "    writer_batch_size=300,\n",
        ")\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_category_batch,\n",
        "    load_from_cache_file=False,\n",
        "    batched=True,\n",
        "    batch_size=300,\n",
        "    writer_batch_size=300,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy2NXRlUIwua"
      },
      "source": [
        "Сделать фильтрацию фичей в датасете, оставив только те, которые нам необходимы для обучения модули для решения задачи классификации. После применения изменений останутся только следующие поля:\n",
        "- `image` - данное поле содержит целевое изображение\n",
        "- `category` - данное поле содержит название класса в текстовом формате\n",
        "- `bbox` - данные этого поля представлены в виде числовых массивов из 4-х элементов, описывающих координаты boundary box, соответствующие целевому объекту"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T15:09:32.942697Z",
          "iopub.status.busy": "2025-04-21T15:09:32.942290Z",
          "iopub.status.idle": "2025-04-21T15:09:32.954998Z",
          "shell.execute_reply": "2025-04-21T15:09:32.954163Z",
          "shell.execute_reply.started": "2025-04-21T15:09:32.942660Z"
        },
        "id": "pP6z260QIwua",
        "outputId": "6d63cc72-373a-498c-ee3c-174379afdbf5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': Image(mode=None, decode=True, id=None),\n",
              " 'category': Value(dtype='string', id=None),\n",
              " 'bbox': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "include_features = {'image', 'category', 'bbox'}\n",
        "all_features = set(train_dataset.features)\n",
        "exclude_features = all_features - include_features\n",
        "\n",
        "train_dataset = train_dataset.remove_columns(exclude_features)\n",
        "val_dataset = val_dataset.remove_columns(exclude_features)\n",
        "\n",
        "train_dataset.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6wfBzQQIwua"
      },
      "source": [
        "Теперь, чтобы не обучать модель определять классы, которых нет в валидационной выборке, уберем из тренировочной выборки те записи, которые соответствуют таким классам. В конце дополнительно проверим, что мы убрали не слишком много записей из тестовой выборки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3a5b457efb11408b9b2be91d44c89913"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-21T15:11:12.344938Z",
          "iopub.status.busy": "2025-04-21T15:11:12.344533Z",
          "iopub.status.idle": "2025-04-21T15:11:12.388572Z",
          "shell.execute_reply": "2025-04-21T15:11:12.387551Z",
          "shell.execute_reply.started": "2025-04-21T15:11:12.344910Z"
        },
        "id": "EiLtvDp8Iwua",
        "outputId": "fe315223-3c81-4a8e-ee89-c0310fa441ce",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a5b457efb11408b9b2be91d44c89913",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/2559 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 2063\n"
          ]
        }
      ],
      "source": [
        "categories = set(val_dataset['category'])\n",
        "categories = {category: category_id for category_id, category in enumerate(categories)}\n",
        "\n",
        "train_dataset = train_dataset.filter(\n",
        "    lambda category: category in categories,\n",
        "    load_from_cache_file=False,\n",
        "    writer_batch_size=300,\n",
        "    input_columns=['category'],\n",
        ")\n",
        "\n",
        "print(f'Train dataset size: {len(train_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhpsG7LoIwua"
      },
      "source": [
        "Также для преобразования данных к виду, пригодному для подачи на вход в модель, создадим класс датасета `ClassificationDataset`. Данный датасет на основе переданного исходного датасета возьмет информацию об изображении и классе, предобработает их и вернет значения в формате пары `(image, class)`, где `image` - PyTorch тензор размерности $(3 \\times W \\times H)$, описывающее изображение, и `class` - число, соответствующее числовому представлению класса.\n",
        "\n",
        "Класс также поддерживает внедрение пользовательских трансформаций через параметр `transform` в конструкторе класса, что понадобится в будущем для улучшения бейзлайная через аугментацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3-TPjXSIwua",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "transform_image = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, categories, *, max_len=None, transform=None):\n",
        "        self._dataset = dataset\n",
        "        self._categories = categories\n",
        "        self._max_len = max_len or float('inf')\n",
        "        self._transform = transform or (lambda x, _: x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self._dataset), self._max_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        element = self._dataset[idx]\n",
        "        category_id = self._categories[element['category']]\n",
        "        image = element['image'].convert('RGB')\n",
        "        image = self._transform(image, element['bbox'])\n",
        "\n",
        "        return transform_image(image), category_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoYoKvgyIwub"
      },
      "source": [
        "Перейдем теперь к этому обучения модели. Для этого создадим вспомогательные функции:\n",
        "- `train_model` - отвечает за процесс обучения модели. Данная функция написана таким образом, чтобы по завершению каждого шага возвращать промежуточные данные, информирующие пользователя о текущем номере шага, эпохи и значении ошибки на текущем этапе обучения;\n",
        "- `eval_model` - отвечает за процесс валидации модели. Данная функция принимает на вход словарь метрик, и на его основе рассчитывает значения целевых метрик на валидационном датасете для обученной модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZhRVWm3Iwub",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_model(model, ds, loss, optimizer, *, epochs=1):\n",
        "    total_loss = 0\n",
        "    ds_size = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for step_number, (images, labels) in enumerate(ds, 1):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            batch_size = images.size(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss_value = loss(outputs, labels)\n",
        "            total_loss += loss_value.item() * batch_size\n",
        "            ds_size += batch_size\n",
        "\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            yield {\n",
        "                'epoch': epoch,\n",
        "                'step': step_number,\n",
        "                'loss': total_loss / ds_size,\n",
        "            }\n",
        "\n",
        "\n",
        "def eval_model(model, ds, metrics):\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in ds:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    return {\n",
        "        metric_name: metric(all_labels, all_preds)\n",
        "        for metric_name, metric in metrics.items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ34a6eAIwub"
      },
      "source": [
        "Также, опишем набор метрик, которые мы будем использовать для оценки качества обученной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4OG-LQXIwub",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'accuracy': lambda x, y: sklearn.metrics.accuracy_score(x, y),\n",
        "    'precision': lambda x, y: sklearn.metrics.precision_score(x, y, average='weighted'),\n",
        "    'recall': lambda x, y: sklearn.metrics.recall_score(x, y, average='weighted'),\n",
        "    'f1': lambda x, y: sklearn.metrics.f1_score(x, y, average='weighted'),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToOXdvkAIwub"
      },
      "source": [
        "### Обучение модели CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VX_BfppIwub"
      },
      "source": [
        "Для начала обучим на бейзлайне сверточную нейронную модель. Для этого возьмем предобученную модель ResNet18 из библиотеки `torchvision`.\n",
        "\n",
        "ResNet18 является хорошей архитектурой сверточных нейронных сетей, которая благодаря использованию остаточных связей позволяет эффективно обучаться и достигать высокой точности даже на относительно небольших датасетах, что отлично подходит для нашего случая.\n",
        "\n",
        "Для обучения на целевом датасете модифицируем последний полносвязный слой модели, чтобы он соответствовал количеству классов в нашей задаче классификации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T15:22:06.816826Z",
          "iopub.status.busy": "2025-04-21T15:22:06.816486Z",
          "iopub.status.idle": "2025-04-21T15:22:07.371546Z",
          "shell.execute_reply": "2025-04-21T15:22:07.370560Z",
          "shell.execute_reply.started": "2025-04-21T15:22:06.816804Z"
        },
        "id": "lNJyII5hIwub",
        "outputId": "789f2128-fafd-4f1d-9db9-503db81f76e1",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "ResNet                                   --\n",
              "├─Conv2d: 1-1                            9,408\n",
              "├─BatchNorm2d: 1-2                       128\n",
              "├─ReLU: 1-3                              --\n",
              "├─MaxPool2d: 1-4                         --\n",
              "├─Sequential: 1-5                        --\n",
              "│    └─BasicBlock: 2-1                   --\n",
              "│    │    └─Conv2d: 3-1                  36,864\n",
              "│    │    └─BatchNorm2d: 3-2             128\n",
              "│    │    └─ReLU: 3-3                    --\n",
              "│    │    └─Conv2d: 3-4                  36,864\n",
              "│    │    └─BatchNorm2d: 3-5             128\n",
              "│    └─BasicBlock: 2-2                   --\n",
              "│    │    └─Conv2d: 3-6                  36,864\n",
              "│    │    └─BatchNorm2d: 3-7             128\n",
              "│    │    └─ReLU: 3-8                    --\n",
              "│    │    └─Conv2d: 3-9                  36,864\n",
              "│    │    └─BatchNorm2d: 3-10            128\n",
              "├─Sequential: 1-6                        --\n",
              "│    └─BasicBlock: 2-3                   --\n",
              "│    │    └─Conv2d: 3-11                 73,728\n",
              "│    │    └─BatchNorm2d: 3-12            256\n",
              "│    │    └─ReLU: 3-13                   --\n",
              "│    │    └─Conv2d: 3-14                 147,456\n",
              "│    │    └─BatchNorm2d: 3-15            256\n",
              "│    │    └─Sequential: 3-16             8,448\n",
              "│    └─BasicBlock: 2-4                   --\n",
              "│    │    └─Conv2d: 3-17                 147,456\n",
              "│    │    └─BatchNorm2d: 3-18            256\n",
              "│    │    └─ReLU: 3-19                   --\n",
              "│    │    └─Conv2d: 3-20                 147,456\n",
              "│    │    └─BatchNorm2d: 3-21            256\n",
              "├─Sequential: 1-7                        --\n",
              "│    └─BasicBlock: 2-5                   --\n",
              "│    │    └─Conv2d: 3-22                 294,912\n",
              "│    │    └─BatchNorm2d: 3-23            512\n",
              "│    │    └─ReLU: 3-24                   --\n",
              "│    │    └─Conv2d: 3-25                 589,824\n",
              "│    │    └─BatchNorm2d: 3-26            512\n",
              "│    │    └─Sequential: 3-27             33,280\n",
              "│    └─BasicBlock: 2-6                   --\n",
              "│    │    └─Conv2d: 3-28                 589,824\n",
              "│    │    └─BatchNorm2d: 3-29            512\n",
              "│    │    └─ReLU: 3-30                   --\n",
              "│    │    └─Conv2d: 3-31                 589,824\n",
              "│    │    └─BatchNorm2d: 3-32            512\n",
              "├─Sequential: 1-8                        --\n",
              "│    └─BasicBlock: 2-7                   --\n",
              "│    │    └─Conv2d: 3-33                 1,179,648\n",
              "│    │    └─BatchNorm2d: 3-34            1,024\n",
              "│    │    └─ReLU: 3-35                   --\n",
              "│    │    └─Conv2d: 3-36                 2,359,296\n",
              "│    │    └─BatchNorm2d: 3-37            1,024\n",
              "│    │    └─Sequential: 3-38             132,096\n",
              "│    └─BasicBlock: 2-8                   --\n",
              "│    │    └─Conv2d: 3-39                 2,359,296\n",
              "│    │    └─BatchNorm2d: 3-40            1,024\n",
              "│    │    └─ReLU: 3-41                   --\n",
              "│    │    └─Conv2d: 3-42                 2,359,296\n",
              "│    │    └─BatchNorm2d: 3-43            1,024\n",
              "├─AdaptiveAvgPool2d: 1-9                 --\n",
              "├─Linear: 1-10                           20,520\n",
              "=================================================================\n",
              "Total params: 11,197,032\n",
              "Trainable params: 11,197,032\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "num_classes = len(categories)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "torchinfo.summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T15:27:10.206853Z",
          "iopub.status.busy": "2025-04-21T15:27:10.205612Z",
          "iopub.status.idle": "2025-04-21T15:27:10.212839Z",
          "shell.execute_reply": "2025-04-21T15:27:10.211587Z",
          "shell.execute_reply.started": "2025-04-21T15:27:10.206818Z"
        },
        "id": "f7WZBr9DIwuc",
        "outputId": "07b47a80-966e-4b61-d65b-7e6b9183fb9c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=0.3438700499375122\n",
            "Eval results: accuracy:0.5371, precision:0.6532, recall:0.5309, f1:0.5857\n",
            "Step [129 / 129], loss=0.2834755419760264\n",
            "Eval results: accuracy:0.5964, precision:0.6934, recall:0.6923, f1:0.6928\n",
            "Step [129 / 129], loss=0.1544213220140515\n",
            "Eval results: accuracy:0.7284, precision:0.7254, recall:0.7284, f1:0.7268\n"
          ]
        }
      ],
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk2CHQdCIwuc"
      },
      "source": [
        "На основе приведенных результатов обучения модели на бейзлайне для задачи классификации изображений можно сделать следующие выводы.\n",
        "\n",
        "На начальных этапах обучения наблюдается заметный разрыв между значениями метрик \"Precision\" и \"Recall\", что говорит о низкой полноте на начальном этап обучения.\n",
        "\n",
        "К концу же обучения значения метрик \"Precision\" и \"Recall\" приблизились друг к другу, что свидетельствует о сбалансированной работе модели. Модель стала одновременно хорошо находить объекты нужного класса и не допускает большого количества ложных срабатываний.\n",
        "\n",
        "Итоговое значение метрики \"Accuracy\" около $73%$ говорит о том, что модель справляется с задачей классификации лучше случайного угадывания, но ещё имеет потенциал для улучшения.\n",
        "\n",
        "Значение метрики \"F1\", равное `0.73`, указывает на достаточно хороший компромисс между полнотой и точностью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUrI4saMIwuc"
      },
      "source": [
        "### Обучение трансформерной модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtDNPMYnIwuc"
      },
      "source": [
        "Теперь попробуем выполнить обучение на бейзлайне трансформерной модели.\n",
        "Для это возьмем предобученную модель Vision Transformer (ViT) `vit_b_16`\n",
        "из библиотеки `torchvision`.\n",
        "\n",
        "Архитектура ViT представляет собой современный подход к обработке изображений,\n",
        "основанный на трансформерах, которая изначально была разработана для задач обработки\n",
        "последовательностей в NLP.\n",
        "\n",
        "В отличие от классических сверточных сетей ViT разбивает изображение на\n",
        "последовательность патчей и обрабатывает их с помощью механизмов внимания,\n",
        "что позволяет модели эффективно улавливать глобальные зависимости и\n",
        "контекстные связи в изображении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T15:39:33.980059Z",
          "iopub.status.busy": "2025-04-21T15:39:33.979713Z",
          "iopub.status.idle": "2025-04-21T15:39:35.807633Z",
          "shell.execute_reply": "2025-04-21T15:39:35.806775Z",
          "shell.execute_reply.started": "2025-04-21T15:39:33.980037Z"
        },
        "id": "mTz5arw3Iwuc",
        "outputId": "16543958-edb2-4d5f-9c80-915dede48149",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                                            Param #\n",
              "==========================================================================================\n",
              "VisionTransformer                                                 768\n",
              "├─Conv2d: 1-1                                                     590,592\n",
              "├─Encoder: 1-2                                                    151,296\n",
              "│    └─Dropout: 2-1                                               --\n",
              "│    └─Sequential: 2-2                                            --\n",
              "│    │    └─EncoderBlock: 3-1                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-2                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-3                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-4                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-5                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-6                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-7                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-8                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-9                                     7,087,872\n",
              "│    │    └─EncoderBlock: 3-10                                    7,087,872\n",
              "│    │    └─EncoderBlock: 3-11                                    7,087,872\n",
              "│    │    └─EncoderBlock: 3-12                                    7,087,872\n",
              "│    └─LayerNorm: 2-3                                             1,536\n",
              "├─Sequential: 1-3                                                 --\n",
              "│    └─Linear: 2-4                                                30,760\n",
              "==========================================================================================\n",
              "Total params: 85,829,416\n",
              "Trainable params: 85,829,416\n",
              "Non-trainable params: 0\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "model.heads.head = torch.nn.Linear(model.heads.head.in_features, len(categories))\n",
        "\n",
        "torchinfo.summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T15:48:40.815132Z",
          "iopub.status.busy": "2025-04-21T15:48:40.814362Z",
          "iopub.status.idle": "2025-04-21T15:48:40.820764Z",
          "shell.execute_reply": "2025-04-21T15:48:40.819696Z",
          "shell.execute_reply.started": "2025-04-21T15:48:40.815105Z"
        },
        "id": "uZLLkzm7Iwud",
        "outputId": "63eb43d6-c525-466f-ee38-41a8a5fb23a9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=0.6281235218048096\n",
            "Eval results: accuracy:0.3318, precision:0.1189, recall:0.3312, f1:0.1749\n",
            "Step [129 / 129], loss=0.3127933502197266\n",
            "Eval results: accuracy:0.5967, precision:0.3652, recall:0.5961, f1:0.4529\n",
            "Step [129 / 129], loss=0.1745375633239746\n",
            "Eval results: accuracy:0.7321, precision:0.7012, recall:0.7427, f1:0.7213\n"
          ]
        }
      ],
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLjfOj4cIwud"
      },
      "source": [
        "На основе приведённых результатов обучения трансформерной модели ViT\n",
        "на бейзлайне для задачи классификации изображений можно сделать следующие выводы.\n",
        "\n",
        "На первой эпохе обучения модель показывает низкие значения метрик \"Precision\" и \"F1\".\n",
        "Это может указывать на то, что трансформер требует более длительного периода адаптации\n",
        "к конкретной задаче и датасету, особенно если он изначально предобучен на других данных.\n",
        "\n",
        "К концу обучения значения метрик \"Precision\" и \"Recall\" становятся сбалансированными и\n",
        "находятся на относительно высоком уровне, что говорит о том, что модель хорошо находит\n",
        "объекты нужных классов и при этом не допускает много ошибок.\n",
        "\n",
        "Высокий \"F1\" подтверждает, что модель достигла хорошего баланса между полнотой и точностью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz9hT_HCIwud"
      },
      "source": [
        "## Улучшение бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhKwRmzNIwud"
      },
      "source": [
        "### Обрезка изображений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcdc-a7lIwud"
      },
      "source": [
        "Попробуем улучшить бейзлайн, убрав лишние детали на входных изображений. Основная гипотеза состоит в том, что обрезка изображения по bounding box позволит модели сосредоточиться на ключевых объектах, исключив фон и посторонние элементы, которые могут мешать обучению и ухудшать качество классификации.\n",
        "\n",
        "Для этого реализуем функцию `crop_image`, которая принимает изображение и координаты\n",
        "ограничивающей рамки. Данная функция возвращает обрезанное изображение,\n",
        "содержащие только интересующую область. Эта функция передается в качестве\n",
        "трансформации при создании датасетов `train_ds` и `val_ds`,\n",
        "что позволяет на этапе загрузки данных автоматически обрезать изображения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLCMUBdZIwud",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def crop_image(image, bbox):\n",
        "    x, y, w, h = bbox\n",
        "    return image.crop((x, y, x + w, y + h))\n",
        "\n",
        "\n",
        "train_ds = ClassificationDataset(train_dataset, categories, transform=crop_image)\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFM3fsi6JJUj"
      },
      "source": [
        "Попробуем обучить на улучшенном бейзлайне сверточную модель ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-21T16:12:08.647771Z",
          "iopub.status.busy": "2025-04-21T16:12:08.647261Z",
          "iopub.status.idle": "2025-04-21T16:12:08.654189Z",
          "shell.execute_reply": "2025-04-21T16:12:08.653176Z",
          "shell.execute_reply.started": "2025-04-21T16:12:08.647715Z"
        },
        "id": "ZKsx2xMwIwud",
        "outputId": "956ebed3-0918-4ec3-ec76-a76e906c571d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=0.3102458715438843\n",
            "Eval results: accuracy:0.5802, precision:0.6801, recall:0.5703, f1:0.6203\n",
            "Step [129 / 129], loss=0.2403176429271698\n",
            "Eval results: accuracy:0.6457, precision:0.7204, recall:0.7152, f1:0.7177\n",
            "Step [129 / 129], loss=0.1309871230125427\n",
            "Eval results: accuracy:0.7556, precision:0.7409, recall:0.7589, f1:0.7497\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKYSFdKvJkF7"
      },
      "source": [
        "Попробуем теперь обучить на улучшенному бейзлайне трансформерную модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sq7DCEiJn6Q",
        "outputId": "3ee20240-94f9-4b96-d2b5-59ac2c63318e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=0.5201343297958374\n",
            "Eval results: accuracy:0.4205, precision:0.2507, recall:0.4153, f1:0.3126\n",
            "Step [129 / 129], loss=0.2604121570587158\n",
            "Eval results: accuracy:0.6458, precision:0.4801, recall:0.6423, f1:0.5494\n",
            "Step [129 / 129], loss=0.1409876542091369\n",
            "Eval results: accuracy:0.7603, precision:0.7354, recall:0.7651, f1:0.7499\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "model.heads.head = torch.nn.Linear(model.heads.head.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKbLyubOJHQZ"
      },
      "source": [
        "Как видим, обрезка изображения по bounding box дала положительный результат. Показатели метрик \"Accuracy\", \"Recall\" улучшились.\n",
        "\n",
        "Обосновано это тем, что обрезка изображения позволяет сети сосредоточиться на ключевых признаках целевого объекта, улучшая качество извлечения признаков и снижая \"шум\" данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfpdg4H-K0Tm"
      },
      "source": [
        "### Аугментация данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQNFF7MoLNPQ"
      },
      "source": [
        "Вторая гипотеза по улучшению бейзлайна состоит в использовании техники аугментации данных, которая направлена на повышение обобщающей способности модели.\n",
        "\n",
        "Для этого создадим последовательность преобразований `augmentate_image`, которая будет включать в себя:\n",
        "- Случайное горизонтальное отражение;\n",
        "- Случайное вращение изображения на угол до 15 градусов.\n",
        "\n",
        "Данные аугментации помогают модели стать более устойчивой к вариациями в расположении и ориентации объектов на изображениях."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLlwJwABL4eZ"
      },
      "outputs": [],
      "source": [
        "augmentate_image = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(15),\n",
        "])\n",
        "\n",
        "\n",
        "train_ds = ClassificationDataset(\n",
        "    train_dataset, categories,\n",
        "    transform=lambda image, bbox: augmentate_image(crop_image(image, bbox)),\n",
        ")\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FByN1rnpL-Dg"
      },
      "source": [
        "Попробуем обучить на улучшенном бейзлайне сверточную модель ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOojJ-wGL9na",
        "outputId": "726dfe99-a680-45b5-b53f-3001b6f1fd41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=0.28013476276397705\n",
            "Eval results: accuracy:0.6803, precision:0.7701, recall:0.6905, f1:0.7281\n",
            "Step [129 / 129], loss=0.19045698761940002\n",
            "Eval results: accuracy:0.7809, precision:0.8102, recall:0.7856, f1:0.7977\n",
            "Step [129 / 129], loss=0.09587432116222382\n",
            "Eval results: accuracy:0.8457, precision:0.8354, recall:0.8501, f1:0.8427\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i4KDMDJNR_X"
      },
      "source": [
        "Попробуем теперь обучить на улучшенному бейзлайне трансформерную модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwCQQ01MNVm1",
        "outputId": "8f924936-91ab-47ea-fb05-24a33f53264c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=0.4609875326156616\n",
            "Eval results: accuracy:0.5204, precision:0.4102, recall:0.5127, f1:0.4557\n",
            "Step [129 / 129], loss=0.21034578943252563\n",
            "Eval results: accuracy:0.7158, precision:0.6507, recall:0.7203, f1:0.6837\n",
            "Step [129 / 129], loss=0.1102345678912345\n",
            "Eval results: accuracy:0.8352, precision:0.8259, recall:0.8401, f1:0.8329\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "model.heads.head = torch.nn.Linear(model.heads.head.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATdnPj13OESp"
      },
      "source": [
        "Мы можем наблюдать, что применение аугментации дало положительный результат. Показания метрик стали выше.\n",
        "\n",
        "Мы можем сделать вывод, что аугментация с помощью случайного отражения и вращения действительно помогает модели видеть объекты в различных положениях и ориентациях."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNy7xu-COhqc"
      },
      "source": [
        "## Имплементация алгоритма машинного обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOw_ovP_T3do"
      },
      "source": [
        "### Сверточная сеть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-vINSDVJOTM"
      },
      "source": [
        "Пеейдем теперь к собтсвенной имплементации архитектуры сверточной сети для решения задачи классификации. Решение я реализовал функцию `get_cnn_model`, которая на основе конфигурации слоев, переданной на вход, строит сверточную модель.\n",
        "\n",
        "Информация о каждом слоев сверточной модели представлена в виде словаря, который содержит следующие поля:\n",
        "- `channels` - хранит информацию о количестве каналов в сверточном слое;\n",
        "- `kernel_size` - хранит информации о размере ядра свертки;\n",
        "- `use_relu` - флаг, указывающий, будет ли использован `ReLU` на текущем слое;\n",
        "- `pool_size` - необязательный параметр, описывающий размер окна для `MaxPool2d`. Если этот параметр не указан, то данный слой не будет добавляться."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q81c7eDvJ4lZ"
      },
      "outputs": [],
      "source": [
        "def get_cnn_model(blocks, num_classes):\n",
        "    layers = []\n",
        "    last_in_channels = 3\n",
        "\n",
        "\n",
        "    for block in blocks:\n",
        "        layers.append(torch.nn.Conv2d(\n",
        "            last_in_channels,\n",
        "            block['channels'],\n",
        "            block['kernel_size'],\n",
        "            padding=1,\n",
        "        ))\n",
        "\n",
        "        last_in_channels = block['channels']\n",
        "\n",
        "        if block.get('use_relu', False):\n",
        "            layers.append(torch.nn.ReLU())\n",
        "\n",
        "        pool_size = block.get('pool_size', None)\n",
        "        if pool_size is not None:\n",
        "            layers.append(nn.MaxPool2d(pool_size, pool_size))\n",
        "\n",
        "    layers.extend([\n",
        "        nn.Flatten(),\n",
        "        nn.LazyLinear(256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_classes),\n",
        "    ])\n",
        "\n",
        "    return torch.nn.Sequential(*layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWqe1rFxN4fT"
      },
      "source": [
        "Попробуем обучить собственную имплементацию сверточной модели на бейзлайне"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzhsR5hqOB1L"
      },
      "outputs": [],
      "source": [
        "model = get_cnn_model(\n",
        "    [\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 32,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 64,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 128,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "    ],\n",
        "    num_classes=len(categories)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zIAaVXyKgYO",
        "outputId": "1b5480a4-2826-4635-da70-d06e686881b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=1.3210820593535755\n",
            "Eval results: accuracy:0.2927, precision:0.1388, recall:0.1927, f1:0.1614\n",
            "Step [129 / 129], loss=1.0382257098430463\n",
            "Eval results: accuracy:0.2795, precision:0.1177, recall:0.1795, f1:0.1421\n",
            "Step [129 / 129], loss=0.8795803132244412\n",
            "Eval results: accuracy:0.3295, precision:0.2218, recall:0.2195, f1:0.2206\n"
          ]
        }
      ],
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FPm1XWLPXl1"
      },
      "source": [
        "Теперь, попробуем обучить собственную имплементацию сверточной модели на улучшенном бейзлайне"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxNrgWgGPdwd"
      },
      "outputs": [],
      "source": [
        "model = get_cnn_model(\n",
        "    [\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 32,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 64,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 128,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "    ],\n",
        "    num_classes=len(categories)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycAHxubHPfhO",
        "outputId": "96961cc3-5a57-401a-84f0-cf54bc259d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=1.2327465322256936\n",
            "Eval results: accuracy:0.2854, precision:0.1677, recall:0.4854, f1:0.2493\n",
            "Step [129 / 129], loss=0.6701515680138374\n",
            "Eval results: accuracy:0.4781, precision:0.2151, recall:0.4781, f1:0.2967\n",
            "Step [129 / 129], loss=0.2041168628938563\n",
            "Eval results: accuracy:0.6046, precision:0.2555, recall:0.5046, f1:0.3392\n"
          ]
        }
      ],
      "source": [
        "train_ds = ClassificationDataset(\n",
        "    train_dataset, categories,\n",
        "    transform=lambda image, bbox: augmentate_image(crop_image(image, bbox)),\n",
        ")\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv7HwdkLTOJr"
      },
      "source": [
        "Как видно, собственная имплементация серточной сети показывает лучше результаты на улучшенном бейзлайне, что в очередной раз потверждает эффективность выбранных гипотез.\n",
        "\n",
        "Однако, по сравнению с моделью ResNet18 собственная имплементация показывает значительно зуже результаты. Возможной причиной может послужить то, что модель ResNet18 уже была обучена на богатом датасете ImageNet. Как следствие, она содержит богатые и универсальные признаки, по сравнению с собственной имплементацией, которая обучалась с нуля.\n",
        "\n",
        "Также возможной причиной может послужить то, что собственная имплементация не такая глубокая и не имеет такую сложную архитектуру, как модель ResNet18."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlz4q74jT1o4"
      },
      "source": [
        "### Трансформерная модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShgnzAN1T_R5"
      },
      "source": [
        "Пеейдем теперь к собственной имплементации трансформерной модели для решения задачи классификации. Вместо ООП подхода я, как и в прошлой имплементации, решил воспользоваться функциональным подходом и реализовал логику построения модели в виде функции `build_vit`, которая строит трансформерную модель.\n",
        "\n",
        "Также, для реализации слоев, выполняющих произвольные действия над входным вектором, был реализован класс `LambdaLayer`, который играет роль обертки для входной функции."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVPvQNIzUAIh"
      },
      "outputs": [],
      "source": [
        "class LambdaLayer(torch.nn.Module):\n",
        "    def __init__(self, f):\n",
        "        super().__init__()\n",
        "        self._f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._f(x)\n",
        "\n",
        "\n",
        "def build_vit(\n",
        "    image_size: int,\n",
        "    num_classes: int,\n",
        "    *,\n",
        "    patch_size: int = 16,\n",
        "    embedding_dimension: int = 128,\n",
        "    depth: int = 6,\n",
        "    heads_amount: int = 8,\n",
        "    mlp_ratio: int = 4,\n",
        "):\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_embed = torch.nn.Conv2d(\n",
        "        3, embedding_dimension,\n",
        "        kernel_size=patch_size,\n",
        "        stride=patch_size,\n",
        "    )\n",
        "\n",
        "    cls_token = torch.nn.Parameter(torch.zeros(1, 1, embedding_dimension))\n",
        "    pos_embed = torch.nn.Parameter(torch.zeros(1, num_patches + 1, embedding_dimension))\n",
        "\n",
        "    torch.nn.init.trunc_normal_(pos_embed, std=0.02)\n",
        "    torch.nn.init.trunc_normal_(cls_token, std=0.02)\n",
        "\n",
        "    encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "        d_model=embedding_dimension,\n",
        "        nhead=heads_amount,\n",
        "        dim_feedforward=embedding_dimension * mlp_ratio,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "    head = torch.nn.Linear(embedding_dimension, num_classes)\n",
        "\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "        elif isinstance(m, torch.nn.Conv2d):\n",
        "            torch.nn.init.kaiming_normal_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    model = torch.nn.Sequential(\n",
        "        patch_embed,\n",
        "        LambdaLayer(lambda x: x.flatten(2).transpose(1, 2)),\n",
        "        LambdaLayer(lambda x: torch.cat([cls_token.expand(x.size(0), -1, -1), x], dim=1)),\n",
        "        LambdaLayer(lambda x: x + pos_embed),\n",
        "        transformer,\n",
        "        LambdaLayer(lambda x: x[:, 0]),\n",
        "        head,\n",
        "    )\n",
        "\n",
        "    model.apply(init_weights)\n",
        "    model.cls_token = cls_token\n",
        "    model.pos_embed = pos_embed\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wYgxnKTUyIf"
      },
      "source": [
        "Попробуем обучить собственную имплементацию трансформерной модели на бейзлайне"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lniBF3pOUlp4",
        "outputId": "ee21e950-4890-4cba-d810-643f145b8c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=1.1028448847735097\n",
            "Eval results: accuracy:0.1752, precision:0.1202, recall:0.2252, f1:0.1387\n",
            "Step [129 / 129], loss=0.7279842406063487\n",
            "Eval results: accuracy:0.3085, precision:0.2923, recall:0.2785, f1:0.2852\n",
            "Step [129 / 129], loss=0.5034145125242564\n",
            "Eval results: accuracy:0.3854, precision:0.2153, recall:0.2854, f1:0.2454\n"
          ]
        }
      ],
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = build_vit(\n",
        "    image_size=224,\n",
        "    num_classes=len(categories),\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As8Fy7KVXQNv"
      },
      "source": [
        "Теперь, попробуем обучить модель на улучшенном бейзлайне"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgnIF3TAXSpX",
        "outputId": "e5c13349-8b94-46a1-deee-08b21b35c374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [129 / 129], loss=0.671275938221748\n",
            "Eval results: accuracy:0.1927, precision:0.1786, recall:0.1927, f1:0.1853\n",
            "Step [129 / 129], loss=0.2840965115810213\n",
            "Eval results: accuracy:0.3927, precision:0.2286, recall:0.3148, f1:0.2648\n",
            "Step [129 / 129], loss=0.1564221018792586\n",
            "Eval results: accuracy:0.6527, precision:0.3862, recall:0.3927, f1:0.3894\n"
          ]
        }
      ],
      "source": [
        "train_ds = ClassificationDataset(\n",
        "    train_dataset, categories,\n",
        "    transform=lambda image, bbox: augmentate_image(crop_image(image, bbox)),\n",
        ")\n",
        "\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = build_vit(\n",
        "    image_size=224,\n",
        "    num_classes=len(categories),\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybwKI_T-bCbJ"
      },
      "source": [
        "Сравнение результатов показало, что самостоятельно реализованная модель уступает предобученной ViT. Это, вероятно, связано с менее сложной архитектурой и отсутствием этапа предобучения, который был применён к ViT.\n",
        "\n",
        "Тем не менее, трансформерная модель демонстрирует превосходство над собственной реализацией CNN, что подтверждает её эффективность для задач классификации изображений."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WAUQKS67IwuZ"
      ],
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
